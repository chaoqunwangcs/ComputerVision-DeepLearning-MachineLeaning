# 1. 有监督学习模型
**决策树、线性模型（LDA）、贝叶斯、KNN、集成学习** \
**决策树**是一种基于规则的方法，它用一组嵌套的规则进行预测，在树的每个决策节点处，根据判断结果进入一个分支，反复执行这种操作直到到达叶子节点，得到决策结果，它的规则是通过训练样本学习得到的，决策树是一种判别模型，也是非线性模型，天然支持多类分类问题。它既可以用于分类问题，也可以用于回归问题，具有很好的解释性，符合人类的思维习惯。常用的决策树有 ID3，C4.5，分类与回归树（CART）等。分类树对应的映射函数是多维空间的分段线性划分，即用平行于各个坐标轴的超平面对空间进行切分；回归树的映射函数是一个分段常数函数。决策树是分段线性函数但不是线性函数，它具有非线性建模的能力。只要划分的足够细，分段常数函数可以逼近闭区间上任意函数到任意指定精度，因此决策树在理论上可以对任意复杂度的数据进行分类或者回归。
**线性模型**的预测函数是线性函数，既可以用于分类问题，也可以用于回归问题，这是机器学习算法中的一个庞大家族。从线性模型中衍生出了多种机器学习算法，对于分类问题，有岭回归，LASSO 回归；对于分类问题，有支持向量机，logistic 回归，softmax 回归，人工神经网络（多层感知器模型），以及后续的各种深度神经网络。
# 1.1 决策树   
决策树(Decision Tree）是在已知各种情况发生概率的基础上，通过构成决策树来求取净现值的期望值大于等于零的概率，评价项目风险，判断其可行性的决策分析方法，是直观运用概率分析的一种图解法  
## ID3 
由增熵（Entropy）原理来决定那个做父节点，那个节点需要分裂。对于一组数据，熵越小说明分类结果越好。熵定义如下：  
$$Entropy = -\sum p_{x_i}*log_{2}p_{x_i}$$  
对于样本的多个属性，选择熵最小的属性（分类效果最好的属性）作为父节点，构建决策树  
## C4.5  
通过对ID3的学习，可以知道ID3存在一个问题，那就是越细小的分割分类错误率越小，所以ID3会越分越细,但是这种分割显然只对训练数据有用，对于新的数据没有意义，这就是所说的过度学习（Overfitting)。
分割太细了，训练数据的分类可以达到0错误率，但是因为新的数据和训练数据不同，所以面对新的数据分错率反倒上升了。决策树是通过分析训练数据，得到数据的统计信息，而不是专为训练数据量身定做。
为了避免分割太细，c4.5对ID3进行了改进，C4.5中，优化项要除以分割太细的代价，这个比值叫做信息增益率，显然分割太细分母增加，信息增益率会降低。除此之外，其他的原理和ID3相同。
$$Info(D)=-\sum p_{x_i}log_2p_{x_i}--(ID3)$$
$$Info_{A}=\sum_{j=1}^v \frac{|D_j|}{|D|} \times Info(D_j)$$
$$gain(A)=Info(D)-Info_{A}(D)$$
$$SplitInfo_{A}(D)=-\sum_{j=1}^v \frac{|D_j|}{|D|} \times log_{2}(\frac {|D_j|}{|D|})$$
$$GainRatio(A)=\frac{Gain(A)}{SplitInfo(A)} --C4.5$$
## CART(Classification And Regression Tree)
ID3中使用了信息增益选择特征，增益大优先选择。C4.5中，采用信息增益比选择特征，减少因特征值多导致信息增益大的问题。CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，不纯度越低，特征越好。这和信息增益（比）相反。
假设K个类别，第k个类别的概率为pk，概率分布的基尼系数表达式：
$$Gini(p)=\sum_{k=1}^K p_k(1-p_k) = 1 - \sum_{k=1}^K p_k^2$$
对于样本D，个数为|D|，假设K个类别，第k个类别的数量为|Ck|，则样本D的基尼系数表达式：
$$Gini(D)=1-\sum_{k=1}^K (\frac{|C_k|}{|D|})^2$$
对于样本D，个数为|D|，根据特征A的某个值a，把D分成|D1|和|D2|，则在特征A的条件下，样本D的基尼系数表达式为：
$$Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2)$$
CART分类树算法每次仅对某个特征的值进行二分，而不是多分，这样CART分类树算法建立起来的是二叉树，而不是多叉树。
**CART回归树**和CART分类树的建立类似，这里只说不同:
1. 分类树与回归树的区别在样本的输出，如果样本输出是离散值，这是分类树；样本输出是连续值，这是回归树。分类树的输出是样本的类别，回归树的输出是一个实数。
2. 连续值的处理方法不同.
3. 决策树建立后做预测的方式不同。

|算法|支持模型|树结构|特征选择|连续值处理|缺失值处理|剪枝|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|ID3|分类|多叉树|信息增益|不支持|不支持|不支持|
|C4.5|分类|多叉树|信息增益比|支持|支持|支持|
|CART|分类回归|二叉树|基尼指数、均方差|支持|支持|支持|

## 剪枝
在决策树的创建时，由于数据中的噪声和离群点，许多分枝反映的是训练数据中的异常。剪枝方法是用来处理这种过分拟合数据的问题。通常剪枝方法都是使用统计度量，剪去最不可靠的分枝。
剪枝一般分两种方法：先剪枝和后剪枝。
**先剪枝**方法中通过提前停止树的构造（比如决定在某个节点不再分裂或划分训练元组的子集）而对树剪枝。一旦停止，这个节点就变成树叶，该树叶可能取它持有的子集最频繁的类作为自己的类。
另一种更常用的方法是**后剪枝**，它由完全成长的树剪去子树而形成。通过删除节点的分枝并用树叶来替换它。树叶一般用子树中最频繁的类别来标记。

## 决策树小结
### 优点
1. 简单直观，生成的决策树很直观。
2. 基本不需要预处理，不需要提前归一化和处理缺失值。
3. 使用决策树预测的代价是O(log2m)。m为样本数。
4. 既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。
5. 可以处理多维度输出的分类问题。
6. 相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以很好解释。
7. 可以交叉验证的剪枝来选择模型，从而提高泛化能力。
8. 对于异常点的容错能力好，健壮性高。
### 缺点
1. 决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。
2. 决策树会因为样本发生一点的改动，导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。 
3. 寻找最优的决策树是一个NP难题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习的方法来改善。
4. 有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。
5. 如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。

# 1.2 线性模型
线性模型的预测函数是线性函数，既可以用于分类问题，也可以用于回归问题，这是机器学习算法中的一个庞大家族。从线性模型中衍生出了多种机器学习算法，对于回归问题，有**岭回归**，***LASSO* 回归**；对于分类问题，有**支持向量机（SVM）**，***logistic* 回归**，***softmax* 回归**，**人工神经网络（多层感知器模型）**，以及后续的各种**深度神经网络**。
对于分类问题，线性模型的预测函数为:
$$f(x)=sgn(w^Tx+b)$$
其中 sgn 是符号函数。最简单的线性分类器是感知器算法.
对于回归问题，线性模型的预测函数为：
$$f(x)=w^Tx+b$$
训练时的目标是最小化均方误差：
$$\min_{w} \frac{1}{2} \sum_{i=1}^{l}(w^Tx_i-y_i)^2$$
可以证明，这是一个凸优化问题，可以得到全局极小值。求解时可以采用梯度下降法或者牛顿法。
## 1.2.1 岭回归
岭回归是线性回归的 L2 正则化版本，训练时求解的问题为:
$$\min_{w} \frac{1}{2} \sum_{i=1}^{l}(w^Tx_i-y_i)^2 + \lambda w_2^2$$
如果系数 l > 0 ，这个问题是一个严格凸优化问题，可用用梯度下降法，牛顿法求解，该公式有[闭式解](https://blog.csdn.net/Joker_sir5/article/details/82756089)。
## 1.2.2 LASSO回归
LASSO回归和岭回归类似，不同的是，Lasso可以理解为在线性回归基础上加入一个L1正则项，同样来限制W不要过大。其中λ>0，通过确定λ的值可以使得模型在偏差和方差之间达到平衡，随着λ的增大，模型的方差减小，偏差增大。
[L1,L2正则化区别](https://blog.csdn.net/wwyy2018/article/details/99765142)

## 1.2.3 线性判别分析LDA 

[传送门](https://www.cnblogs.com/pinard/p/6244265.html) 
LDA是一种有监督的线性投影技术，它寻找向低维空间的投影矩阵 W，样本的特征向量 x 经过投影之后得到的新向量 y：
$$y=Wx$$
投影的目标是同一类样投影后的结果向量同类**样本差异尽可能小，不同类的样本差异尽可能大**。直观来看，就是经过这个投影之后同一类的样本进来聚集在一起，不同类的样本尽可能离得远。对于**二分类问题**：

$$arg \min_{w} J(w)=\frac{||w^T\mu_0 - w^T\mu_1||_2^2}{w^T\sum_0w+w^T\sum_1w} = \frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}{w^T(\sum_0+\sum_1)w}$$
定义类内散度矩阵：
$$S_w=\sum_0+\sum_1=\sum_{x \in X_0}(x-\mu_0)(x-\mu_0)^T +\sum_{x \in X_1}(x-\mu_1)(x-\mu_1)^T$$
定义类间散度矩阵：
$$S_b=(\mu_0-\mu_1)(\mu_0-\mu_1)^T$$
于是，优化目标转化为：
$$\arg \min_{w} J(w) = \frac{w^TS_bw}{w^TS_ww}$$
可以求得：
$$w=S_w^{-1}(\mu_0-\mu_1)$$
**LDA&&PCA**
均为降维方法，相同点：
1. 两者均可以对数据进行降维。
2. 两者在降维时均使用了矩阵特征分解的思想。
3. 两者都假设数据符合高斯分布。
不同点：
1. LDA是有监督的降维方法，而PCA是无监督的降维方法
2. LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。
3. LDA除了可以用于降维，还可以用于分类。
4. LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。

**LDA小结**
LDA算法既可以用来降维，又可以用来分类，但是目前来说，主要还是用于降维。在我们进行图像识别图像识别相关的数据分析时，LDA是一个有力的工具。

优点:
1. 在降维过程中可以使用类别的先验知识经验，而像PCA这样的无监督学习则无法使用类别先验知识。
2. LDA在样本分类信息依赖均值而不是方差的时候，比PCA之类的算法较优。

缺点：
1. LDA不适合对非高斯分布样本进行降维，PCA也有这个问题。
2. LDA降维最多降到类别数k-1的维数，如果我们降维的维度大于k-1，则不能使用LDA。当然目前有一些LDA的进化版算法可以绕过这个问题。
3. LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好。
4. LDA可能过度拟合数据。

## 1.2.4 支持向量机（SVM）

[传送门](https://zhuanlan.zhihu.com/p/31886934)
支持向量机（support vector machines, SVM）是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；SVM还包括核技巧，这使它成为实质上的非线性分类器。SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。

SVM学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。如下图所示， $wx+b=0$ 即为分离超平面，对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机），但是几何间隔最大的分离超平面却是唯一的。
定义训练集
$$T = {(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$$
$x_i \in R^n, y_i \in {+1, -1}, i = 1,2,...N$。 假设样本点$(x_i,y_i)$和超平面$wx+b=0$的集合间隔为：
$$\gamma_i = y_i(\frac{w}{||w||}\cdot x_i + \frac{b}{||w||})$$
于是SVM可以表示为以下约束最优化问题：
$$\max_{w,b} \gamma $$
$$s.t. y_i(\frac{w}{||w||}\cdot x_i + \frac{b}{||w||}) >= \gamma, i=1,2,...,N$$
向量机的一个重要性质：训练完成后，大部分的训练样本都不需要保留，最终模型仅与支持向量有关。

## 1.2.5 logistic 回归

[传送门](https://www.cnblogs.com/zm-pop-pk/p/11668294.html)

### logit变换

我们在建立回归方程时，因变量的取值范围为实数集；而在我们所研究的问题中，这些分类型因变量的取值却是在0~1之间，如患病率为0.1、0.5、0.8等等，因此需要先对因变量的值（目标概率）做logit变换。
设事件发生的概率为p，不发生的概率为1−p，则将$\frac{p}{1-p}$称为事件的发生比，记为odds（比数、优势），logit变换即为:
$$logit(p)=ln(\frac{p}{1-p})$$
显然，当p=1时，logit(p)取值为+∞；当p=0.5时，logit(p)=0；当p=0时，logit(p)取值为−∞。这样一来，就把因变量的取值范围从0~1扩展到了实数集，而采用了这种处理的回归分析就是logistic回归。

### logistic回归模型
设有一个二分类因变量y，取值为1时表示事件发生，取值为0时表示事件未发生；该因变量有m个影响因素（自变量）：$x_1,x_2,...,x_m$；记事件发生的条件概率$P(y=1∣x_i)=p_i$，则由$p_i$（第i个观测）所构建的logistic回归模型为：
$$logit(p_i)=ln(\frac{p_i}{1-p_i})=\beta_0+\beta_1x_1+\cdots +\beta_mx_m=\beta_0+\sum_{j=1}^{m}\beta_jx_j,j=1,2,\cdots,m
$$
其中$\beta_j$表示自变量$x_j$改变一个单位时，$logit(p_i)$的改变量，可以理解为各个影响因素的权重系数。
通过变换，logistic回归模型也可以写成如下形式：
$$p_i=\frac{e^{\beta_0+\sum_{j=1}^{m}\beta_jx_j}}{1+e^{\beta_0+\sum_{j=1}^{m}\beta_jx_j}}$$
通过观察logistic回归模型，我们会发现它与线性回归模型非常相似。事实上，logistic回归模型属于广义线性模型（generalized linear model）。